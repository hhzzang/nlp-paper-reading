## kakaobrain/nlp-paper-reading Papers

- [Balancing Training for Multilingual NMT](notes//Balancing_Training_for_Multilingual_NMT.md)
- [TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data](notes//TaBERT.md)
- [Optimizing Data Usage via Differentiable Rewards](notes/Optimizing_Data_Usage_via_Differentiable_Rewards.md)
- [Language-agnostic BERT Sentence Embedding](notes/LaBSE.md)
- [PLATO & PLATO-2](notes/PLATO.md)
- [GPT-3 (English)](notes/GPT-3.md)
- [XLU: XNLI, XLM, XLM-R (English)](notes/XLU.md)
- [Meena (English)](notes/Meena.md)

## [clova paper-reading list](https://clova-ai.blog/publication-list/)

- GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation  
- Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search
- Reward Optimization for Neural Machine Translation with Learned Metrics
- Integration of Pre-trained Networks with Continuous Token Interface for End-to-End Spoken Language Understanding
- Rethinking Channel Dimensions for Efficient Model Design
- Probabilistic Embeddings for Cross-Modal Retrieval
- Designing a Minimal Retrieve-and-Read System for Open-Domain Question Answering
- Two-stage Textual Knowledge Distillation to Speech Encoder for Spoken Language Understanding
- ST-BERT: Cross-modal Language Model Pre-training For End-to-end Spoken Language Understanding
- NN-KOG2P: A Novel Grapheme-Phoneme model for Korean language
- Show, Attend and Distill: Knowledge Distillation via Attention-based Feature Matching
- DialogBERT: Discourse-Aware Response Generation via Learning to Recover and Rank Utterances
- AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights
- Scale down Transformer by Grouping Features for a Lightweight Character-level Language Model
- Large Product Key Memory for Pretrained Language Models
- Variational Hierarchical Dialog Autoencoder for Dialog State Tracking Data Augmentation
- Context-Aware Answer Extraction in Question Answering


## [황승원 교수님 paper-reading list](http://dilab.yonsei.ac.kr/publications.php)


- C2L: Causally Contrastive Learning for Robust Text Classification. AAAI 2022
- Conditional Response Augmentation for Dialogue using Knowledge Distillation.
- SQuAD2-CR: Semi-supervised Annotation for Cause and Rationales for Unanswerability in SQuAD 2.0.
- BERT Is NOT All You Need for Commonsense Inference. 
- Structure-Augmented Keyphrase Generation. 
- SCOPA: Soft Code-Switching and Pairwise Alignment for Zero-Shot Cross-lingual Transfer. 


## [오혜연 교수님 paper-reading list](http://uilab.kr/)
- Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning
- Mitigating Language-Dependent Ethnic Bias in BERT
- Knowledge-Enhanced Evidence Retrieval for Counterargument Generation

## [맹성현 교수님 paper-reading list](http://ir.kaist.ac.kr/research/papers/)

-  Can You Distinguish Truthful from Fake Reviews? User Analysis and Assistance Tool for Fake Review Detection
-  Leveraging Order-Free Tag Relations for Context-Aware Recommendation
-  Have You Seen That Number? Investigating Extrapolation in Question Answering Models 
-  Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval
